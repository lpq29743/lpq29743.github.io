---
layout: post
title: PyTorch 入门
categories: DeepLearning
description: PyTorch入门
keywords: PyTorch, 深度学习, 深度学习框架, 神经网络
---

用了 TensorFlow 半年，现在要开始来踩 PyTorch 的坑了！

与 TensorFlow 类似，PyTorch 中的核心也是 Tensor。先来看 PyTorch 下定义 Tensor 的一些方法：

```python
import torch

torch.empty(5, 3) # 创建一个空矩阵
torch.ones(5, 3)  # 创建一个元素都为 1 的矩阵
torch.zeros(5, 3)  # 创建一个元素都为 0 的矩阵
torch.eye(4)  # 创建一个 4 * 4 的单位矩阵
torch.rand(5, 3)  # 创建一个元素在 [0, 1) 中随机分布的矩阵
torch.randn(5, 3)  # 创建一个符合标准正态分布的矩阵
torch.tensor([5.5, 3]) # 创建一个具体的矩阵
```

再继续看 Tensor 的一些操作：

```python
import torch

# PyTorch 加法的多种实现方式
result = x + y
result = torch.add(x, y)
torch.add(x, y, out = result)
y.add_(x)
# 在某一维度连接两个张量
torch.cat((x, y), 1)
# 去掉长度为 1 的某一维度
m.squeeze(1)
# 增加某一个维度
m.unsqueeze(1)
# 相当于 TensorFlow 的 reshape 操作
m.view(5, 3, -1)
# 扩展张量
m.expand(5, 3)
# 移到 GPU 上运算
m.cuda()
# 移到 CPU 上运算
m.cpu()
# Numpy 和 Tensor 的相互转换
torch.from_numpy(n)
m.numpy()
```

上面都是常量的定义，接下来看看变量的定义：

```python
from torch.autograd import Variable

m = torch.ones(5, 3)
v = Variable(m)
```

Variable 有两个重要的属性：

- requires_grad：指定是否更新此参数，对于不需要更新的变量可以设定为 False，从而加快运算
- volatile：指定是否保留记录，当设置为 True 时，表示不记录运算，可加快运算，当此值为 True 时，requires_grad 一定是 False

接下来我们通过一个简单的程序了解 PyTorch 中训练模型的大致步骤：

```python
import torch
from torch.autograd import Variable
from torch.optim import SGD

m1 = torch.ones(5, 3)
m2 = torch.ones(5, 3)

a = Variable(m1, requires_grad=True)
b = Variable(m2, requires_grad=True)

optimizer = SGD([a, b], lr=0.1)

for _ in range(10):
    loss = (a + b).sum()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()  
```

实际上，PyTorch 一般都是用 Module 来组织代码，具体使用可参考如下：

```python
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
       x = F.relu(self.conv1(x))
       return F.relu(self.conv2(x))
       

model = Model()
y = model(_input)
```

